{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "\n",
    "# For modeling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, precision_recall_curve\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load the processed data\n",
    "print(\"Loading data...\")\n",
    "data_path = Path('../data/processed/telco_churn_for_modeling.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()\n",
    "\n",
    "# Define features and target\n",
    "print(\"\\nPreparing features and target...\")\n",
    "# Drop unnecessary columns\n",
    "drop_cols = ['CustomerID', 'Count', 'Quarter', 'Churn Label', 'Churn Value', \n",
    "             'Churn Score Category', 'CLTV Category', 'Churn Category', 'Churn Reason',\n",
    "             'Satisfaction Score Label', 'Customer Status', 'Lat Long']\n",
    "\n",
    "X = df.drop(drop_cols + ['Churn'], axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical features: {len(categorical_features)}\")\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "\n",
    "# Create preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model performance and display results.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'model': model\n",
    "    }\n",
    "\n",
    "# 1. Logistic Regression Model\n",
    "print(\"\\nTraining Logistic Regression model...\")\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "lr_param_grid = {\n",
    "    'classifier__C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "lr_grid_search = GridSearchCV(\n",
    "    lr_pipeline,\n",
    "    param_grid=lr_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {lr_grid_search.best_params_}\")\n",
    "lr_best_model = lr_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "lr_results = evaluate_model(lr_best_model, X_test, y_test, \"Logistic Regression\")\n",
    "\n",
    "# 2. Random Forest Model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "rf_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5],\n",
    "    'classifier__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    rf_pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {rf_grid_search.best_params_}\")\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_results = evaluate_model(rf_best_model, X_test, y_test, \"Random Forest\")\n",
    "\n",
    "# 3. XGBoost Model\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "xgb_param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [3, 5, 7],\n",
    "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'classifier__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "print(f\"Best parameters: {xgb_grid_search.best_params_}\")\n",
    "xgb_best_model = xgb_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate XGBoost\n",
    "xgb_results = evaluate_model(xgb_best_model, X_test, y_test, \"XGBoost\")\n",
    "\n",
    "# Compare models\n",
    "models = {\n",
    "    'Logistic Regression': lr_results,\n",
    "    'Random Forest': rf_results,\n",
    "    'XGBoost': xgb_results\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(models.keys()),\n",
    "    'Accuracy': [models[m]['accuracy'] for m in models],\n",
    "    'Precision': [models[m]['precision'] for m in models],\n",
    "    'Recall': [models[m]['recall'] for m in models],\n",
    "    'F1 Score': [models[m]['f1'] for m in models],\n",
    "    'ROC AUC': [models[m]['roc_auc'] for m in models]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "comparison_df.set_index('Model')\n",
    "\n",
    "# Visualize model comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    sns.barplot(x='Model', y=metric, data=comparison_df, palette='viridis')\n",
    "    plt.title(f'Model Comparison - {metric}')\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify the best model based on ROC AUC\n",
    "best_model_name = comparison_df.loc[comparison_df['ROC AUC'].idxmax(), 'Model']\n",
    "best_model = models[best_model_name]['model']\n",
    "print(f\"\\nBest performing model: {best_model_name} (ROC AUC: {models[best_model_name]['roc_auc']:.4f})\")\n",
    "\n",
    "# Feature importance for the best model (if applicable)\n",
    "if best_model_name in ['Random Forest', 'XGBoost']:\n",
    "    # Get feature names after preprocessing\n",
    "    preprocessor = best_model.named_steps['preprocessor']\n",
    "    feature_names = []\n",
    "    \n",
    "    # Get numerical feature names (unchanged)\n",
    "    feature_names.extend(numerical_features)\n",
    "    \n",
    "    # Get one-hot encoded feature names\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "    categorical_features_encoded = ohe.get_feature_names_out(categorical_features)\n",
    "    feature_names.extend(categorical_features_encoded)\n",
    "    \n",
    "    # Get feature importances\n",
    "    if best_model_name == 'Random Forest':\n",
    "        importances = best_model.named_steps['classifier'].feature_importances_\n",
    "    else:  # XGBoost\n",
    "        importances = best_model.named_steps['classifier'].feature_importances_\n",
    "    \n",
    "    # Create a dataframe of feature importances\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False).head(20)\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis')\n",
    "    plt.title(f'Top 20 Feature Importances - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Save the best model\n",
    "print(f\"\\nSaving the best model ({best_model_name})...\")\n",
    "joblib.dump(best_model, models_dir / 'best_churn_model.joblib')\n",
    "print(f\"Model saved to {models_dir / 'best_churn_model.joblib'}\")\n",
    "\n",
    "# Create a simple model card\n",
    "model_card = f\"\"\"\n",
    "# Churn Prediction Model Card\n",
    "\n",
    "## Model Details\n",
    "- **Model Type:** {best_model_name}\n",
    "- **Version:** 1.0\n",
    "- **Date Created:** {pd.Timestamp.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "## Performance Metrics\n",
    "- **Accuracy:** {models[best_model_name]['accuracy']:.4f}\n",
    "- **Precision:** {models[best_model_name]['precision']:.4f}\n",
    "- **Recall:** {models[best_model_name]['recall']:.4f}\n",
    "- **F1 Score:** {models[best_model_name]['f1']:.4f}\n",
    "- **ROC AUC:** {models[best_model_name]['roc_auc']:.4f}\n",
    "\n",
    "## Intended Use\n",
    "- **Primary Use Case:** Predict customer churn for telecom services\n",
    "- **Intended Users:** Business analysts, customer retention teams\n",
    "\n",
    "## Training Data\n",
    "- **Source:** Telco Customer Churn dataset\n",
    "- **Size:** {X_train.shape[0]} training samples, {X_test.shape[0]} test samples\n",
    "- **Features:** {X.shape[1]} features (after preprocessing)\n",
    "\n",
    "## Ethical Considerations\n",
    "- The model should be used as a decision support tool, not as the sole basis for customer interventions.\n",
    "- Regular monitoring is required to ensure the model remains fair and unbiased across different customer segments.\n",
    "\n",
    "## Limitations\n",
    "- The model is trained on historical data and may not capture new or emerging churn patterns.\n",
    "- Performance may vary across different customer segments.\n",
    "\n",
    "## Recommendations\n",
    "- Deploy model in a monitoring framework to track performance over time.\n",
    "- Retrain periodically with fresh data to maintain accuracy.\n",
    "- Use model predictions alongside domain expertise for retention strategies.\n",
    "\"\"\"\n",
    "\n",
    "# Save the model card\n",
    "with open(models_dir / 'model_card.md', 'w') as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"Model card created and saved.\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\nModel Training Summary:\")\n",
    "print(f\"1. Best performing model: {best_model_name}\")\n",
    "print(f\"2. ROC AUC score: {models[best_model_name]['roc_auc']:.4f}\")\n",
    "print(\"3. Model and documentation saved to the models directory\")\n",
    "print(\"4. Next steps: Implement the model in the Flask API for real-time predictions\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
